<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Face Emotion Recognition using Deep Learning</title>
        {% load static %}
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.1.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="{% static 'css/styles.css' %}" rel="stylesheet" />
    </head>
    <body>
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light" id="mainNav">
            <div class="container px-4 px-lg-5">
                <a class="navbar-brand" href="index.html">Projects</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <em class="fas fa-bars"></em>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto py-4 py-lg-0">
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="index.html">Home</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="about.html">About</a></li>

                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="contact.html">Contact</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Page Header-->
        <header class="masthead" style="background-image: url('static/images/fer.jpg')">
            <div class="container position-relative px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <div class="post-heading">
                            <h1>Face Emotion Recognition using Deep Learning</h1>
                            <h2 class="subheading">If someone showed you a picture of a person and asked you to guess what they’re feeling, chances are you’d have a pretty good idea about it.</h2>
                            <span class="meta">
                                Posted by
                                <a href="#!">Mohit Kumar</a>
                                on May 30, 2022
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </header>
        <!-- Post Content-->
        <article class="mb-4">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <p>The following blog contains a recollection of all of the series of things that we did during the project with InspiritAI, while it does show a lot of the code required to do Emotion Detection, this is by no means the fastest way to train a model. Multiple ML and AI models were used to see the difference between them.

The three main components of Emotion Detection are as follows:

    Image Preprocessing,
    Feature Extraction,and
    Feature Classification</p>
                        <p>Facial detection is an important step in emotion detection. It removes the parts of the image that aren’t relevant. Here’s one way of detecting faces in images.</p>
                        <p>Another way to do this is by using dlib’s pre-trained face detector model which is used in the next point as well.</p>
                        <p>One way to differentiate between two emotions is to see whether the persons mouth and eyes are open or not. We can find the euclidian distance between the points specifically on the mouth, if a person is surprised, the distance would be more than it would be if they’re not..</p>
                        <p>For those who have seen the Earth from space, and for the hundreds and perhaps thousands more who will, the experience most certainly changes your perspective. The things that we share in our world are far more valuable than those which divide us.</p>
                        <h2 class="section-heading">Facial Detection</h2>
                        <p>Facial detection is the first part of our pipeline. We have used the python library Face Recognition that we found easy to install and very accurate in detecting faces. This library scans the input image and returns the bounding box coordinates of all detected faces as shown below:</p>
                        <p>Facial landmarks are a set of key points on human face images. The points are defined by their (x,y) coordinates on the image. These points are used to locate and represent salient regions of the face, such as eyes, eyebrows, nose, mouth and jawline.</p>
                        <blockquote class="blockquote">The facial landmark model we used was the Dlib’s pre-trained Facial Landmark Detection Model which detects 68 2-Dimensional points on the human face.</blockquote>
                        <p>To improve performance you can very the dropout, number of dense layers and activation functions. We also used transfer learning with a CNN called VGG which is a pre-trained convolutional neural network for image classification.</p>
                        <h2 class="section-heading">Emotion Detection</h2>
                        <p>Humans are used to taking in non verbal cues from facial emotions. Now computers are also getting better to reading emotions. So how do we detect emotions in an image? We have used an open source data set — Face Emotion Recognition (FER) from Kaggle and built a CNN to detect emotions. The emotions can be classified into 7 classes — happy, sad, fear, disgust, angry, neutral and surprise.</p>
                        <a href="#!"><img class="img-fluid" src="static/images/fer2.png" alt="..." /></a>
                        <span class="caption text-muted">A demo representation of software.</span>
                        <p>The best results came by using the VGG which were right about 68–70% of the time, but even the linear models did a very good job. Although 50% accuracy does not seem like a lot, it is still more than if you would pick up a picture and a label at random. There, you would be right about 20% of the time.</p>
                        <p>The VGG, however, performs even better than humans do in this particular dataset. The difference between the CNNs and the MLPs was that the CNNs were extracting features that they deemed important by themselves, while we were feeding either pixels or landmarks as features to the MLPs</p>
                        <p>

                            &middot; Images by
                            <a href="#">Mohit Kumar</a>
                        </p>
                    </div>
                </div>
            </div>
        </article>
        <!-- Footer-->
        <footer class="border-top">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <ul class="list-inline text-center">
                            <li class="list-inline-item">
                                <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <em class="fas fa-circle fa-stack-2x"></em>
                                        <em class="fab fa-twitter fa-stack-1x fa-inverse"></em>
                                    </span>
                                </a>
                            </li>
                            <li class="list-inline-item">
                                <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <em class="fas fa-circle fa-stack-2x"></em>
                                        <em class="fab fa-facebook-f fa-stack-1x fa-inverse"></em>
                                    </span>
                                </a>
                            </li>
                            <li class="list-inline-item">
                                <a href="https://github.com/megatrongodlike"  target="_blank">
                                    <span class="fa-stack fa-lg">
                                        <em class="fas fa-circle fa-stack-2x"></em>
                                        <em class="fab fa-github fa-stack-1x fa-inverse"></em>
                                    </span>
                                </a>
                            </li>
                        </ul>
                        <div class="small text-center text-muted fst-italic">Copyright &copy; RedEyeVFX 2022</div>
                    </div>
                </div>
            </div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    <script>window.addEventListener('DOMContentLoaded', () => {
    let scrollPos = 0;
    const mainNav = document.getElementById('mainNav');
    const headerHeight = mainNav.clientHeight;
    window.addEventListener('scroll', function() {
        const currentTop = document.body.getBoundingClientRect().top * -1;
        if ( currentTop < scrollPos) {
            // Scrolling Up
            if (currentTop > 0 && mainNav.classList.contains('is-fixed')) {
                mainNav.classList.add('is-visible');
            } else {
                console.log(123);
                mainNav.classList.remove('is-visible', 'is-fixed');
            }
        } else {
            // Scrolling Down
            mainNav.classList.remove(['is-visible']);
            if (currentTop > headerHeight && !mainNav.classList.contains('is-fixed')) {
                mainNav.classList.add('is-fixed');
            }
        }
        scrollPos = currentTop;
    });
})</script>
    </body>
</html>
